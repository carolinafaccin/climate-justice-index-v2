{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b06df19",
   "metadata": {},
   "source": [
    "Código consolidado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a20d94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INICIANDO PROCESSAMENTO INTEGRADO (SC + H3) ---\n",
      "\n",
      "1. Processando Favelas a partir de: br_setores.gpkg\n",
      "-> Favelas processadas. Linhas: 468099\n",
      "\n",
      "2. Lendo CSVs do Censo IBGE...\n",
      "-> Encontrados 13 arquivos CSV.\n",
      "-> Consolidação concluída. Total colunas: 21\n",
      "\n",
      "3. Calculando índices Setoriais...\n",
      "\n",
      "4. Salvando arquivos de Setores Censitários...\n",
      "-> SC Vulnerabilidade salvo (CSV/Parquet). Shape: (468099, 5)\n",
      "-> SC Interseccionalidade salvo (CSV/Parquet). Shape: (468099, 5)\n",
      "\n",
      "5. Processando malha H3...\n",
      "-> H3 Vulnerabilidade salvo: ..\\data\\interim\\norm_winsorized\\br_h3_vulnerabilidade.parquet | Shape: (4572203, 5)\n",
      "-> H3 Interseccionalidade salvo: ..\\data\\interim\\norm_winsorized\\br_h3_interseccionalidade.parquet | Shape: (4572203, 5)\n",
      "\n",
      "--- PROCESSO CONCLUÍDO COM SUCESSO ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURAÇÕES E CAMINHOS\n",
    "# ==============================================================================\n",
    "\n",
    "# Caminhos de Entrada\n",
    "# Ajuste se necessário, considerando onde você roda o script\n",
    "path_csvs_ibge = Path('../data/raw/ibge/censo/2022/agregados_por_setores/csv/')\n",
    "input_gpkg_favelas = Path('../data/raw/ibge/censo/2022/setores_censitarios/br_setores.gpkg')\n",
    "path_base_h3 = Path('../data/interim/norm_winsorized/br_h3_res9.parquet')\n",
    "\n",
    "# Caminhos de Saída (Pasta única)\n",
    "output_dir = Path('../data/interim/norm_winsorized')\n",
    "\n",
    "# Chaves de Ligação\n",
    "csv_key = 'cd_setor'  # Padronizado para minúsculo\n",
    "geo_key = 'cd_setor'\n",
    "\n",
    "# Lista de TODAS as variáveis necessárias (EM MINÚSCULO)\n",
    "required_vars = [\n",
    "    # Demografia básica\n",
    "    'v00001', 'v01006', \n",
    "    # V1: Renda\n",
    "    'v06004',\n",
    "    # V3: Saneamento\n",
    "    'v00201', 'v00238', \n",
    "    # V4: Educação\n",
    "    'v00853', 'v00855', 'v00857',\n",
    "    # I1: Gênero\n",
    "    'v01042', \n",
    "    # I2: Faixa etária\n",
    "    'v01063', 'v01031', 'v01041', \n",
    "    # I3: Cor e Raça\n",
    "    'v01317', 'v01318', 'v01319', 'v01320', 'v01321', \n",
    "    # Comunidades Tradicionais\n",
    "    'v01500', 'v03000'\n",
    "]\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. FUNÇÕES AUXILIARES\n",
    "# ==============================================================================\n",
    "\n",
    "def ensure_folder(file_path):\n",
    "    \"\"\"Cria a pasta do arquivo se ela não existir.\"\"\"\n",
    "    if file_path.suffix: # É arquivo\n",
    "        file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    else: # É pasta\n",
    "        file_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def normalize_direct(series, lower_limit=0.01, upper_limit=0.99):\n",
    "    \"\"\"\n",
    "    Aplica Winsorization (corte de outliers) e depois normaliza de 0 a 1.\n",
    "    \"\"\"\n",
    "    # Garante numérico e preenche NaN com 0\n",
    "    series = pd.to_numeric(series, errors='coerce').fillna(0)\n",
    "    \n",
    "    # Calcula os limites (percentis 1% e 99%)\n",
    "    lower_val = series.quantile(lower_limit)\n",
    "    upper_val = series.quantile(upper_limit)\n",
    "    \n",
    "    # Faz o 'clip' (Winsorization)\n",
    "    series_clipped = series.clip(lower=lower_val, upper=upper_val)\n",
    "    \n",
    "    # Normalização Min-Max nos dados já cortados\n",
    "    min_val = series_clipped.min()\n",
    "    max_val = series_clipped.max()\n",
    "    \n",
    "    if max_val == min_val:\n",
    "        return 0\n",
    "        \n",
    "    return (series_clipped - min_val) / (max_val - min_val)\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. EXECUÇÃO\n",
    "# ==============================================================================\n",
    "\n",
    "def main():\n",
    "    print(\"--- INICIANDO PROCESSAMENTO INTEGRADO (SC + H3) ---\")\n",
    "    ensure_folder(output_dir)\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # A. PROCESSAR FAVELAS (Gera indicador v2)\n",
    "    # --------------------------------------------------------------------------\n",
    "    print(f\"\\n1. Processando Favelas a partir de: {input_gpkg_favelas.name}\")\n",
    "    \n",
    "    try:\n",
    "        # Lê apenas colunas essenciais do GPKG\n",
    "        gdf_setores = gpd.read_file(input_gpkg_favelas, ignore_geometry=True)\n",
    "        \n",
    "        # Garante que as colunas existam e estejam em minúsculo\n",
    "        gdf_setores.columns = [c.lower() for c in gdf_setores.columns]\n",
    "        \n",
    "        # Prepara dataframe de favelas\n",
    "        # cd_fcu é o código da favela. Se existir, é favela.\n",
    "        if 'cd_fcu' in gdf_setores.columns:\n",
    "            df_fav = gdf_setores[[geo_key, 'cd_fcu']].copy()\n",
    "            df_fav['v2_mor_norm'] = np.where(df_fav['cd_fcu'].notnull(), 1, 0)\n",
    "        else:\n",
    "            print(\"AVISO: Coluna 'cd_fcu' não encontrada. Criando v2_mor_norm zerado.\")\n",
    "            df_fav = gdf_setores[[geo_key]].copy()\n",
    "            df_fav['v2_mor_norm'] = 0\n",
    "\n",
    "        # Garante tipo string para merge\n",
    "        df_fav[geo_key] = df_fav[geo_key].astype(str)\n",
    "        df_fav = df_fav[[geo_key, 'v2_mor_norm']].drop_duplicates(subset=[geo_key])\n",
    "        \n",
    "        print(f\"-> Favelas processadas. Linhas: {len(df_fav)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERRO CRÍTICO ao processar favelas: {e}\")\n",
    "        return\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # B. LER E AGREGAR CSVs DO CENSO\n",
    "    # --------------------------------------------------------------------------\n",
    "    print(\"\\n2. Lendo CSVs do Censo IBGE...\")\n",
    "    \n",
    "    # Começamos o DataFrame base com os dados da Favela\n",
    "    df_base = df_fav.copy()\n",
    "    \n",
    "    csv_files = glob.glob(str(path_csvs_ibge / \"*.csv\"))\n",
    "    print(f\"-> Encontrados {len(csv_files)} arquivos CSV.\")\n",
    "\n",
    "    for f in csv_files:\n",
    "        try:\n",
    "            # Lê cabeçalho para checar colunas\n",
    "            # Tenta separador ; primeiro\n",
    "            df_header = pd.read_csv(f, nrows=0, sep=';', encoding='utf-8')\n",
    "            if len(df_header.columns) <= 1: # Fallback para vírgula\n",
    "                df_header = pd.read_csv(f, nrows=0, sep=',', encoding='utf-8')\n",
    "            \n",
    "            # Normaliza nomes de colunas para minúsculo para comparação\n",
    "            file_cols_lower = [c.lower() for c in df_header.columns]\n",
    "            \n",
    "            # Verifica interseção com variáveis requeridas\n",
    "            vars_in_file = [v for v in required_vars if v in file_cols_lower]\n",
    "            \n",
    "            if vars_in_file and csv_key in file_cols_lower:\n",
    "                # Mapeia colunas originais para leitura\n",
    "                # Precisamos saber o nome original (maiúsculo/minúsculo) para o usecols\n",
    "                orig_cols_map = {c.lower(): c for c in df_header.columns}\n",
    "                cols_to_read = [orig_cols_map[csv_key]] + [orig_cols_map[v] for v in vars_in_file]\n",
    "                \n",
    "                # Lê o arquivo\n",
    "                try:\n",
    "                    df_temp = pd.read_csv(f, usecols=cols_to_read, sep=';', dtype=str)\n",
    "                except:\n",
    "                    df_temp = pd.read_csv(f, usecols=cols_to_read, sep=',', dtype=str)\n",
    "                \n",
    "                # Padroniza nomes para minúsculo\n",
    "                df_temp.columns = [c.lower() for c in df_temp.columns]\n",
    "                \n",
    "                # Garante chave string\n",
    "                df_temp[geo_key] = df_temp[geo_key].astype(str)\n",
    "                \n",
    "                # Remove duplicatas\n",
    "                df_temp = df_temp.drop_duplicates(subset=[geo_key])\n",
    "                \n",
    "                # Converte dados para numérico (forçando erros a virar NaN)\n",
    "                for col in vars_in_file:\n",
    "                    df_temp[col] = pd.to_numeric(df_temp[col].str.replace(',', '.'), errors='coerce')\n",
    "                \n",
    "                # Merge no DataFrame base\n",
    "                df_base = pd.merge(df_base, df_temp, on=geo_key, how='outer')\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao ler {os.path.basename(f)}: {e}\")\n",
    "\n",
    "    # Preencher NAs com 0\n",
    "    df_base = df_base.fillna(0)\n",
    "    print(f\"-> Consolidação concluída. Total colunas: {len(df_base.columns)}\")\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # C. CÁLCULOS DOS ÍNDICES (SETORES)\n",
    "    # --------------------------------------------------------------------------\n",
    "    print(\"\\n3. Calculando índices Setoriais...\")\n",
    "\n",
    "    total_dom = df_base['v00001'].replace(0, np.nan) # Domicílios\n",
    "    total_pop = df_base['v01006'].replace(0, np.nan) # População\n",
    "\n",
    "    # --- VULNERABILIDADE ---\n",
    "    # v1: Renda (Invertido: Renda Alta = Vul Baixa)\n",
    "    if 'v06004' in df_base.columns:\n",
    "        df_base['v1_ren'] = df_base['v06004']\n",
    "        v1_norm_calc = normalize_direct(df_base['v1_ren'])\n",
    "        df_base['v1_ren_norm'] = 1.0 - v1_norm_calc\n",
    "    \n",
    "    # v2: Moradia (Já calculado na etapa A como v2_mor_norm)\n",
    "    \n",
    "    # v3: Infraestrutura (Água + Esgoto) / Domicílios\n",
    "    if 'v00201' in df_base.columns and 'v00238' in df_base.columns:\n",
    "        df_base['v3_inf'] = (df_base['v00201'] + df_base['v00238']) / total_dom\n",
    "        df_base['v3_inf_norm'] = normalize_direct(df_base['v3_inf'].fillna(0))\n",
    "\n",
    "    # v4: Educação (Analfabetismo) / População\n",
    "    if 'v00853' in df_base.columns:\n",
    "        df_base['v4_edu'] = (df_base['v00853'] + df_base['v00855'] + df_base['v00857']) / total_pop\n",
    "        df_base['v4_edu_norm'] = normalize_direct(df_base['v4_edu'].fillna(0))\n",
    "\n",
    "    # --- INTERSECCIONALIDADE ---\n",
    "    # i1: Gênero (Chefe Mulher) / Domicílios\n",
    "    if 'v01063' in df_base.columns:\n",
    "        df_base['i1_gen'] = df_base['v01063'] / total_dom\n",
    "        df_base['i1_gen_norm'] = normalize_direct(df_base['i1_gen'].fillna(0))\n",
    "\n",
    "    # i2: Faixa Etária (Jovens + Idosos) / População\n",
    "    if 'v01031' in df_base.columns:\n",
    "        df_base['i2_fax'] = (df_base['v01031'] + df_base['v01041']) / total_pop\n",
    "        df_base['i2_fax_norm'] = normalize_direct(df_base['i2_fax'].fillna(0))\n",
    "\n",
    "    # i3: Cor (Não Brancos) / População\n",
    "    if 'v01317' in df_base.columns:\n",
    "        # v01317 são os brancos. (Total - Brancos) = Pretos, Pardos, Indígenas, Amarelos\n",
    "        df_base['i3_cor'] = (total_pop - df_base['v01317']) / total_pop\n",
    "        df_base['i3_cor_norm'] = normalize_direct(df_base['i3_cor'].fillna(0))\n",
    "\n",
    "    # i4: Comunidades Tradicionais (Indigenas + Quilombolas) / Domicílios\n",
    "    if 'v01500' in df_base.columns:\n",
    "        df_base['i4_com'] = (df_base['v01500'] + df_base['v03000']) / total_dom\n",
    "        df_base['i4_com_norm'] = normalize_direct(df_base['i4_com'].fillna(0))\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # D. SALVAR ARQUIVOS SETORIAIS (SC)\n",
    "    # --------------------------------------------------------------------------\n",
    "    print(\"\\n4. Salvando arquivos de Setores Censitários...\")\n",
    "\n",
    "    # Definir colunas finais\n",
    "    cols_vul_export = [geo_key, 'v1_ren_norm', 'v2_mor_norm', 'v3_inf_norm', 'v4_edu_norm']\n",
    "    cols_int_export = [geo_key, 'i1_gen_norm', 'i2_fax_norm', 'i3_cor_norm', 'i4_com_norm']\n",
    "\n",
    "    # Criar DataFrames finais (com dropna para limpar sujeira se houver, mas mantendo 0s)\n",
    "    df_vul_final = df_base[[c for c in cols_vul_export if c in df_base.columns]].fillna(0)\n",
    "    df_int_final = df_base[[c for c in cols_int_export if c in df_base.columns]].fillna(0)\n",
    "\n",
    "    # Paths de Saída\n",
    "    out_sc_vul_csv = output_dir / 'br_sc_vulnerabilidade.csv'\n",
    "    out_sc_vul_parq = output_dir / 'br_sc_vulnerabilidade.parquet'\n",
    "    \n",
    "    out_sc_int_csv = output_dir / 'br_sc_interseccionalidade.csv'\n",
    "    out_sc_int_parq = output_dir / 'br_sc_interseccionalidade.parquet'\n",
    "\n",
    "    # Salvar Vulnerabilidade\n",
    "    df_vul_final.to_csv(out_sc_vul_csv, index=False)\n",
    "    df_vul_final.to_parquet(out_sc_vul_parq, index=False)\n",
    "    print(f\"-> SC Vulnerabilidade salvo (CSV/Parquet). Shape: {df_vul_final.shape}\")\n",
    "\n",
    "    # Salvar Interseccionalidade\n",
    "    df_int_final.to_csv(out_sc_int_csv, index=False)\n",
    "    df_int_final.to_parquet(out_sc_int_parq, index=False)\n",
    "    print(f\"-> SC Interseccionalidade salvo (CSV/Parquet). Shape: {df_int_final.shape}\")\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # E. PROCESSAR E SALVAR H3 (HEXÁGONOS)\n",
    "    # --------------------------------------------------------------------------\n",
    "    print(\"\\n5. Processando malha H3...\")\n",
    "\n",
    "    if not path_base_h3.exists():\n",
    "        print(f\"ERRO: Base H3 não encontrada em {path_base_h3}. Pule esta etapa.\")\n",
    "    else:\n",
    "        # Lê base H3\n",
    "        df_h3 = pd.read_parquet(path_base_h3)\n",
    "        # Garante chave string para o merge\n",
    "        df_h3['cd_setor'] = df_h3['cd_setor'].astype(str)\n",
    "\n",
    "        # Merge H3 + Vulnerabilidade (Inner join: só hexágonos que têm setor)\n",
    "        df_h3_vul = pd.merge(df_h3[['h3_id', 'cd_setor']], df_vul_final, on='cd_setor', how='inner')\n",
    "        # Limpa colunas duplicadas ou desnecessárias se houver\n",
    "        df_h3_vul = df_h3_vul.drop(columns=['cd_setor']) \n",
    "        \n",
    "        # Merge H3 + Interseccionalidade\n",
    "        df_h3_int = pd.merge(df_h3[['h3_id', 'cd_setor']], df_int_final, on='cd_setor', how='inner')\n",
    "        df_h3_int = df_h3_int.drop(columns=['cd_setor'])\n",
    "\n",
    "        # Paths de Saída H3\n",
    "        out_h3_vul = output_dir / 'br_h3_vulnerabilidade.parquet'\n",
    "        out_h3_int = output_dir / 'br_h3_interseccionalidade.parquet'\n",
    "\n",
    "        # Salvar\n",
    "        df_h3_vul.to_parquet(out_h3_vul, index=False)\n",
    "        print(f\"-> H3 Vulnerabilidade salvo: {out_h3_vul} | Shape: {df_h3_vul.shape}\")\n",
    "\n",
    "        df_h3_int.to_parquet(out_h3_int, index=False)\n",
    "        print(f\"-> H3 Interseccionalidade salvo: {out_h3_int} | Shape: {df_h3_int.shape}\")\n",
    "\n",
    "    print(\"\\n--- PROCESSO CONCLUÍDO COM SUCESSO ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
