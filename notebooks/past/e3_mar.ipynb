{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8edef353",
   "metadata": {},
   "source": [
    "### 1. Download dos arquivos tif do Anadem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1ad8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvando em: c:\\Users\\CarolinaFaccin\\OneDrive - World Resources Institute\\Carolina WRI\\Repositorios\\climate-justice-index\\data\\raw\\ana\\anadem\\original\n",
      "\n",
      "Arquivo já completo: anadem_v1_20P.tif\n",
      "Arquivo já completo: anadem_v1_21P.tif\n",
      "Arquivo já completo: anadem_v1_21N.tif\n",
      "Arquivo já completo: anadem_v1_22M.tif\n",
      "Arquivo já completo: anadem_v1_22N.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "anadem_v1_23M.tif: 100%|██████████| 1.51G/1.51G [01:54<00:00, 1.77MiB/s]\n",
      "anadem_v1_24M.tif:  55%|█████▍    | 603M/1.07G [01:03<04:33, 1.90MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conexão caiu (ChunkedEncodingError). Retomando em 10s... (1/50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "anadem_v1_24M.tif: 100%|██████████| 1.07G/1.07G [04:13<00:00, 2.04MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo já completo: anadem_v1_25M.tif\n",
      "Arquivo já completo: anadem_v1_25L.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "anadem_v1_24L.tif:  23%|██▎       | 349M/1.47G [00:34<12:17, 1.64MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conexão caiu (ChunkedEncodingError). Retomando em 10s... (1/50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "anadem_v1_24L.tif:  55%|█████▌    | 835M/1.47G [04:49<06:38, 1.76MiB/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conexão caiu (ChunkedEncodingError). Retomando em 10s... (2/50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "anadem_v1_24L.tif:  92%|█████████▏| 1.35G/1.47G [04:51<01:07, 1.96MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conexão caiu (ChunkedEncodingError). Retomando em 10s... (3/50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "anadem_v1_24L.tif: 100%|██████████| 1.47G/1.47G [01:06<00:00, 1.99MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo já completo: anadem_v1_24K.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "anadem_v1_23K.tif:  22%|██▏       | 428M/1.91G [03:40<14:30, 1.84MiB/s]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conexão caiu (ChunkedEncodingError). Retomando em 10s... (1/50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "anadem_v1_23K.tif:  52%|█████▏    | 0.98G/1.91G [04:51<07:54, 2.09MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conexão caiu (ChunkedEncodingError). Retomando em 10s... (2/50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "anadem_v1_23K.tif:  78%|███████▊  | 1.48G/1.91G [04:48<04:03, 1.86MiB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conexão caiu (ChunkedEncodingError). Retomando em 10s... (3/50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "anadem_v1_23K.tif: 100%|██████████| 1.91G/1.91G [03:38<00:00, 2.07MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo já completo: anadem_v1_23J.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "anadem_v1_22J.tif:  42%|████▏     | 638M/1.47G [01:14<08:15, 1.84MiB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conexão caiu (ChunkedEncodingError). Retomando em 10s... (1/50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "anadem_v1_22J.tif:  75%|███████▌  | 1.11G/1.47G [04:48<03:33, 1.82MiB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conexão caiu (ChunkedEncodingError). Retomando em 10s... (2/50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "anadem_v1_22J.tif: 100%|██████████| 1.47G/1.47G [03:10<00:00, 2.04MiB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo já completo: anadem_v1_22H.tif\n",
      "\n",
      "Processo finalizado!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import urllib3\n",
    "from http.client import IncompleteRead\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Desabilita avisos chatos de SSL se houver\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "def download_file_resume(url, save_path, max_retries=50): # Aumentei muito os retries\n",
    "    \"\"\"\n",
    "    Baixa um arquivo com suporte a resume e é extremamente persistente.\n",
    "    \"\"\"\n",
    "    # Define modo de escrita (append se já existe, write se é novo)\n",
    "    if os.path.exists(save_path):\n",
    "        current_size = os.path.getsize(save_path)\n",
    "        resume_header = {'Range': f'bytes={current_size}-'}\n",
    "        temp_mode = 'ab'\n",
    "    else:\n",
    "        resume_header = {}\n",
    "        current_size = 0\n",
    "        temp_mode = 'wb'\n",
    "\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            # 1. Tenta pegar o tamanho total do arquivo\n",
    "            try:\n",
    "                head_resp = requests.head(url, timeout=30)\n",
    "                if head_resp.status_code == 404:\n",
    "                    print(f\"ERRO 404: Arquivo não existe -> {url}\")\n",
    "                    return False\n",
    "                total_file_size = int(head_resp.headers.get('content-length', 0))\n",
    "            except:\n",
    "                # Se falhar no HEAD, tentamos baixar direto para ver o que dá\n",
    "                total_file_size = 0\n",
    "\n",
    "            # Se já baixamos tudo, retorna sucesso\n",
    "            if current_size >= total_file_size and total_file_size > 0:\n",
    "                print(f\"Arquivo já completo: {os.path.basename(save_path)}\")\n",
    "                return True\n",
    "\n",
    "            # 2. Configura headers e faz o request\n",
    "            headers = resume_header.copy()\n",
    "            response = requests.get(url, headers=headers, stream=True, timeout=60)\n",
    "            \n",
    "            # Se servidor reiniciou o download (200) em vez de continuar (206)\n",
    "            if response.status_code == 200:\n",
    "                temp_mode = 'wb'\n",
    "                current_size = 0\n",
    "            elif response.status_code not in [200, 206]:\n",
    "                raise requests.exceptions.RequestException(f\"Status ruim: {response.status_code}\")\n",
    "\n",
    "            # 3. Baixa o conteúdo\n",
    "            desc = os.path.basename(save_path)\n",
    "            total_bar = total_file_size if total_file_size > 0 else None\n",
    "            \n",
    "            with open(save_path, temp_mode) as file, tqdm(\n",
    "                desc=desc,\n",
    "                total=total_bar,\n",
    "                initial=current_size,\n",
    "                unit='iB',\n",
    "                unit_scale=True,\n",
    "                unit_divisor=1024,\n",
    "            ) as bar:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        size = file.write(chunk)\n",
    "                        bar.update(size)\n",
    "            \n",
    "            # Se chegou aqui, sucesso!\n",
    "            return True\n",
    "\n",
    "        # --- BLOCO DE TRATAMENTO DE ERROS CORRIGIDO ---\n",
    "        except (requests.exceptions.RequestException, \n",
    "                urllib3.exceptions.ProtocolError,\n",
    "                urllib3.exceptions.ReadTimeoutError,\n",
    "                IncompleteRead,\n",
    "                ConnectionResetError) as e:\n",
    "            \n",
    "            print(f\"\\nConexão caiu ({type(e).__name__}). Retomando em 10s... ({retries+1}/{max_retries})\")\n",
    "            retries += 1\n",
    "            time.sleep(10) # Espera um pouco mais para a rede estabilizar\n",
    "            \n",
    "            # Atualiza o ponteiro para retomar o download\n",
    "            if os.path.exists(save_path):\n",
    "                current_size = os.path.getsize(save_path)\n",
    "                resume_header = {'Range': f'bytes={current_size}-'}\n",
    "                temp_mode = 'ab'\n",
    "\n",
    "        # --- O GOLEIRO: Pega qualquer outro erro inesperado ---\n",
    "        except Exception as e:\n",
    "            print(f\"\\nErro genérico não tratado ({e}). Tentando novamente em 10s...\")\n",
    "            retries += 1\n",
    "            time.sleep(10)\n",
    "            # Tenta retomar mesmo assim\n",
    "            if os.path.exists(save_path):\n",
    "                current_size = os.path.getsize(save_path)\n",
    "                resume_header = {'Range': f'bytes={current_size}-'}\n",
    "                temp_mode = 'ab'\n",
    "\n",
    "    print(f\"Falha definitiva após {max_retries} tentativas.\")\n",
    "    return False\n",
    "\n",
    "def processar_lista(tiles_list, base_url, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    print(f\"Salvando em: {os.path.abspath(output_folder)}\\n\")\n",
    "\n",
    "    for tile in tiles_list:\n",
    "        file_name = f\"anadem_v1_{tile}.tif\"\n",
    "        url = f\"{base_url}{file_name}\"\n",
    "        save_path = os.path.join(output_folder, file_name)\n",
    "        \n",
    "        success = download_file_resume(url, save_path)\n",
    "        if not success:\n",
    "            print(f\"--> ERRO CRÍTICO: Não foi possível baixar {file_name}.\\n\")\n",
    "\n",
    "# --- CONFIGURAÇÃO ---\n",
    "\n",
    "lista_tiles = [\n",
    "    \"20P\", \"21P\", \"21N\", \"22M\", \"22N\", \"23M\", \"24M\", \"25M\", \n",
    "    \"25L\", \"24L\", \"24K\", \"23K\", \"23J\", \"22J\", \"22H\"\n",
    "]\n",
    "\n",
    "url_base = \"https://metadados.snirh.gov.br/files/anadem_v1_tiles/\"\n",
    "# Ajuste o caminho se necessário\n",
    "pasta_destino = os.path.join(\"..\", \"data\", \"raw\", \"ana\", \"anadem\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    processar_lista(lista_tiles, url_base, pasta_destino)\n",
    "    print(\"\\nProcesso finalizado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfcb5c1",
   "metadata": {},
   "source": [
    "### 2. Identificar municípios defronte com o mar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210a7c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Lendo arquivo CSV de municípios costeiros...\n",
      "2. Lendo arquivo GPKG dos Setores Censitários (Isso pode demorar um pouco)...\n",
      "   - Total de setores carregados: 472780\n",
      "3. Realizando o cruzamento (Left Join)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CarolinaFaccin\\AppData\\Local\\Temp\\ipykernel_22788\\2688384629.py:46: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  gdf_merged['mun_defronte_mar'] = gdf_merged[col_alvo_csv].fillna(False).astype(bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Salvando novo arquivo GPKG...\n",
      "--- Sucesso! ---\n",
      "Arquivo salvo em: ..\\data\\clean\\sc\\BR_setores_CD2022_mar.gpkg\n",
      "Verificação rápida:\n",
      "    CD_MUN  mun_defronte_mar\n",
      "0  1100015             False\n",
      "1  1100015             False\n",
      "2  1100015             False\n",
      "3  1100015             False\n",
      "4  1100015             False\n",
      "\n",
      "Contagem de setores em municípios costeiros: 83940\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# --- CONFIGURAÇÃO DOS CAMINHOS ---\n",
    "# Usando os caminhos relativos que você forneceu\n",
    "input_gpkg = os.path.join(\"..\", \"data\", \"raw\", \"ibge\", \"BR_setores_CD2022_v2.gpkg\")\n",
    "input_csv = os.path.join(\"..\", \"data\", \"raw\", \"ibge\", \"malha_municipal_2024\", \"municipios_defrontantes_com_o_mar.csv\")\n",
    "output_gpkg = os.path.join(\"..\", \"data\", \"clean\", \"sc\", \"BR_setores_CD2022_mar.gpkg\")\n",
    "\n",
    "def processar_cruzamento():\n",
    "    print(\"1. Lendo arquivo CSV de municípios costeiros...\")\n",
    "    # Lemos forçando o CD_MUN como string para garantir o match com o GPKG\n",
    "    df_mar = pd.read_csv(\n",
    "        input_csv, \n",
    "        sep=';', \n",
    "        dtype={'CD_MUN': str} \n",
    "    )\n",
    "    \n",
    "    # Garantir que a coluna alvo está limpa (caso venha como string \"True\")\n",
    "    col_alvo_csv = 'municipios_defrontantes_com_o_mar'\n",
    "    \n",
    "    # Se o CSV tiver apenas os True, o resto será NaN no join.\n",
    "    # Vamos manter apenas as colunas necessárias para o join ficar leve\n",
    "    df_mar = df_mar[['CD_MUN', col_alvo_csv]]\n",
    "\n",
    "    print(\"2. Lendo arquivo GPKG dos Setores Censitários (Isso pode demorar um pouco)...\")\n",
    "    gdf_setores = gpd.read_file(input_gpkg)\n",
    "    \n",
    "    # Garantir que CD_MUN do GeoDataFrame também seja string\n",
    "    if 'CD_MUN' in gdf_setores.columns:\n",
    "        gdf_setores['CD_MUN'] = gdf_setores['CD_MUN'].astype(str)\n",
    "    else:\n",
    "        raise ValueError(\"A coluna CD_MUN não foi encontrada no GPKG.\")\n",
    "\n",
    "    print(f\"   - Total de setores carregados: {len(gdf_setores)}\")\n",
    "\n",
    "    print(\"3. Realizando o cruzamento (Left Join)...\")\n",
    "    # Join pela coluna CD_MUN\n",
    "    gdf_merged = gdf_setores.merge(df_mar, on='CD_MUN', how='left')\n",
    "\n",
    "    # Tratamento da nova coluna:\n",
    "    # Onde deu match é True, onde não deu (NaN) vira False\n",
    "    gdf_merged['mun_defronte_mar'] = gdf_merged[col_alvo_csv].fillna(False).astype(bool)\n",
    "\n",
    "    # Remove a coluna original do CSV para não ficar duplicada ou com nome feio\n",
    "    gdf_merged = gdf_merged.drop(columns=[col_alvo_csv])\n",
    "\n",
    "    print(\"4. Salvando novo arquivo GPKG...\")\n",
    "    # Salvando no novo caminho\n",
    "    gdf_merged.to_file(output_gpkg, driver='GPKG')\n",
    "    \n",
    "    print(f\"--- Sucesso! ---\")\n",
    "    print(f\"Arquivo salvo em: {output_gpkg}\")\n",
    "    print(f\"Verificação rápida:\")\n",
    "    print(gdf_merged[['CD_MUN', 'mun_defronte_mar']].head())\n",
    "    print(f\"\\nContagem de setores em municípios costeiros: {gdf_merged['mun_defronte_mar'].sum()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    processar_cruzamento()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca57ead",
   "metadata": {},
   "source": [
    "### 3. Criar o Mosaico Virtual (VRT) dos tiles do Anadem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd9c3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encontrados 15 arquivos TIF.\n",
      "Mosaico virtual criado em: ..\\data\\raw\\ana\\anadem\\virtual\\anadem_litoral_virtual.vrt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from osgeo import gdal\n",
    "\n",
    "# Caminhos\n",
    "input_folder = os.path.join(\"..\", \"data\", \"raw\", \"ana\", \"anadem\", \"original\")\n",
    "output_vrt = os.path.join(\"..\", \"data\", \"raw\", \"ana\", \"anadem\", \"virtual\", \"anadem_litoral_virtual.vrt\")\n",
    "\n",
    "# Lista todos os TIFs baixados\n",
    "tif_files = glob.glob(os.path.join(input_folder, \"*.tif\"))\n",
    "\n",
    "print(f\"Encontrados {len(tif_files)} arquivos TIF.\")\n",
    "\n",
    "# Cria o VRT (Virtual Raster)\n",
    "# Isso leva segundos, pois não processa pixels, apenas metadados.\n",
    "gdal.BuildVRT(output_vrt, tif_files)\n",
    "\n",
    "print(f\"Mosaico virtual criado em: {output_vrt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3261fc",
   "metadata": {},
   "source": [
    "### 4. Setores: Loop para calcular a elevação do nivel do mar e inputar os dados nos setores censitários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f760e0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando setores censitários...\n",
      "Total de municípios costeiros para processar: 279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando Municípios: 100%|██████████| 279/279 [15:03<00:00,  3.24s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidando resultados...\n",
      "Backup CSV salvo em: ..\\data\\clean\\sc\\temp_resultados_e3.csv\n",
      "Criando GPKG final (E3)...\n",
      "Sucesso! Arquivo final salvo em: ..\\data\\clean\\sc\\BR_setores_CD2022_e3.gpkg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from rasterio.features import shapes\n",
    "from shapely.geometry import shape, box\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- CONFIGURAÇÕES ---\n",
    "vrt_path = os.path.join(\"..\", \"data\", \"raw\", \"ana\", \"anadem\", \"virtual\", \"anadem_litoral_virtual.vrt\")\n",
    "input_setores = os.path.join(\"..\", \"data\", \"clean\", \"sc\", \"BR_setores_CD2022_mar.gpkg\")\n",
    "# REMOVIDO: output_parquet (não vamos mais usar)\n",
    "output_gpkg = os.path.join(\"..\", \"data\", \"clean\", \"sc\", \"BR_setores_CD2022_e3.gpkg\")\n",
    "\n",
    "def calcular_exposicao_costeira():\n",
    "    print(\"Carregando setores censitários...\")\n",
    "    gdf_all = gpd.read_file(input_setores)\n",
    "    \n",
    "    if 'CD_MUN' in gdf_all.columns:\n",
    "        gdf_all['CD_MUN'] = gdf_all['CD_MUN'].astype(str)\n",
    "\n",
    "    # Filtrar apenas setores de municípios costeiros\n",
    "    gdf_costa = gdf_all[gdf_all['mun_defronte_mar'] == True].copy()\n",
    "    gdf_costa = gdf_costa.reset_index(drop=True)\n",
    "    \n",
    "    municipios_unicos = gdf_costa['CD_MUN'].unique()\n",
    "    print(f\"Total de municípios costeiros para processar: {len(municipios_unicos)}\")\n",
    "\n",
    "    resultados_lista = []\n",
    "\n",
    "    with rasterio.open(vrt_path) as src_raster:\n",
    "        for cd_mun in tqdm(municipios_unicos, desc=\"Processando Municípios\"):\n",
    "            try:\n",
    "                gdf_mun = gdf_costa[gdf_costa['CD_MUN'] == cd_mun].copy()\n",
    "                if gdf_mun.empty: continue\n",
    "\n",
    "                # Recorte e Identificação de Risco\n",
    "                bbox_mun = box(*gdf_mun.total_bounds)\n",
    "                try:\n",
    "                    out_image, out_transform = mask(src_raster, [bbox_mun], crop=True)\n",
    "                except ValueError:\n",
    "                    continue \n",
    "\n",
    "                dados_altimetria = out_image[0]\n",
    "                mask_risco = (dados_altimetria <= 1.0) & (dados_altimetria > -50)\n",
    "\n",
    "                if not mask_risco.any(): continue\n",
    "\n",
    "                geoms_risco = []\n",
    "                for geom, val in shapes(mask_risco.astype('int16'), mask=mask_risco, transform=out_transform):\n",
    "                    geoms_risco.append(shape(geom))\n",
    "                \n",
    "                if not geoms_risco: continue\n",
    "\n",
    "                gdf_risco = gpd.GeoDataFrame({'geometry': geoms_risco}, crs=gdf_mun.crs)\n",
    "                gdf_mask = gdf_risco.dissolve()\n",
    "                gdf_mask['geometry'] = gdf_mask.geometry.buffer(0)\n",
    "\n",
    "                # Intersecção\n",
    "                interseccao = gpd.overlay(gdf_mun.reset_index(drop=True), \n",
    "                                          gdf_mask.reset_index(drop=True), \n",
    "                                          how='intersection')\n",
    "\n",
    "                if interseccao.empty: continue\n",
    "\n",
    "                # Cálculos de Área\n",
    "                gdf_mun_proj = gdf_mun.to_crs(epsg=5880)\n",
    "                areas_totais = gdf_mun_proj.set_index('CD_SETOR').geometry.area\n",
    "\n",
    "                inter_proj = interseccao.to_crs(epsg=5880)\n",
    "                inter_proj['area_alagada_m2'] = inter_proj.geometry.area\n",
    "                areas_alagadas = inter_proj.groupby('CD_SETOR')['area_alagada_m2'].sum()\n",
    "\n",
    "                for cd_setor, area_alagada in areas_alagadas.items():\n",
    "                    area_total = areas_totais.get(cd_setor, np.nan)\n",
    "                    pct = 0.0\n",
    "                    if pd.notna(area_total) and area_total > 0:\n",
    "                        pct = (area_alagada / area_total) * 100\n",
    "                        pct = min(pct, 100.0)\n",
    "\n",
    "                    resultados_lista.append({\n",
    "                        'CD_SETOR': cd_setor,\n",
    "                        'area_alagada_m2': area_alagada,\n",
    "                        'pct_area_impactada_1m': pct\n",
    "                    })\n",
    "\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "    # --- SALVAMENTO (MODIFICADO) ---\n",
    "    print(\"Consolidando resultados...\")\n",
    "    if not resultados_lista:\n",
    "        print(\"Nenhum resultado gerado.\")\n",
    "        return\n",
    "\n",
    "    df_resultados = pd.DataFrame(resultados_lista)\n",
    "    \n",
    "    # Agrega duplicatas\n",
    "    df_resultados = df_resultados.groupby('CD_SETOR', as_index=False).agg({\n",
    "        'area_alagada_m2': 'max',\n",
    "        'pct_area_impactada_1m': 'max'\n",
    "    })\n",
    "    \n",
    "    # 1. Backup em CSV (Muito mais seguro que Parquet)\n",
    "    csv_backup = os.path.join(\"..\", \"data\", \"clean\", \"sc\", \"temp_resultados_e3.csv\")\n",
    "    df_resultados.to_csv(csv_backup, sep=';', index=False)\n",
    "    print(f\"Backup CSV salvo em: {csv_backup}\")\n",
    "\n",
    "    print(\"Criando GPKG final (E3)...\")\n",
    "    \n",
    "    # Garante tipagem para o merge\n",
    "    gdf_all['CD_SETOR'] = gdf_all['CD_SETOR'].astype(str)\n",
    "    df_resultados['CD_SETOR'] = df_resultados['CD_SETOR'].astype(str)\n",
    "\n",
    "    gdf_final = gdf_all.merge(df_resultados, on='CD_SETOR', how='left')\n",
    "    gdf_final[['pct_area_impactada_1m', 'area_alagada_m2']] = gdf_final[['pct_area_impactada_1m', 'area_alagada_m2']].fillna(0)\n",
    "    \n",
    "    gdf_final.to_file(output_gpkg, driver='GPKG')\n",
    "    print(f\"Sucesso! Arquivo final salvo em: {output_gpkg}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    calcular_exposicao_costeira()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b25bd19",
   "metadata": {},
   "source": [
    "### 5. Setores: Update do '_e3.gpkg'\n",
    "Este código pega o seu arquivo já pronto e zera os valores dos setores que estão longe da costa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62931694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Lendo arquivo alvo: ..\\data\\clean\\sc\\BR_setores_CD2022_e3.gpkg ...\n",
      "2. Calculando filtro de conectividade com o oceano...\n",
      "   - Setores validados como costeiros: 2043\n",
      "3. Criando novas colunas...\n",
      "   - Valores zerados por estarem no interior: 4085 setores.\n",
      "4. Sobrescrevendo arquivo: ..\\data\\clean\\sc\\BR_setores_CD2022_e3.gpkg ...\n",
      "--- Sucesso! ---\n",
      "Novas colunas adicionadas:\n",
      "1. 'pct_area_impactada_1m_limpo' (Percentual filtrado)\n",
      "2. 'e3_mar_norm' (Normalizado 0-1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- CONFIGURAÇÕES ---\n",
    "# O arquivo alvo (que será lido e sobrescrito)\n",
    "arquivo_alvo = os.path.join(\"..\", \"data\", \"clean\", \"sc\", \"BR_setores_CD2022_e3.gpkg\")\n",
    "input_oceano = os.path.join(\"..\", \"data\", \"raw\", \"ibge\", \"hidrografia_2017\", \"oceano_atlantico.gpkg\")\n",
    "\n",
    "def atualizar_arquivo_existente():\n",
    "    print(f\"1. Lendo arquivo alvo: {arquivo_alvo} ...\")\n",
    "    gdf_setores = gpd.read_file(arquivo_alvo)\n",
    "    \n",
    "    # Garante que CD_SETOR é string para evitar problemas de compatibilidade\n",
    "    if 'CD_SETOR' in gdf_setores.columns:\n",
    "        gdf_setores['CD_SETOR'] = gdf_setores['CD_SETOR'].astype(str)\n",
    "\n",
    "    # --- ETAPA DE FILTRAGEM (Spatial Filter) ---\n",
    "    print(\"2. Calculando filtro de conectividade com o oceano...\")\n",
    "    \n",
    "    # Seleciona apenas setores com algum risco > 0 para testar a proximidade (otimização)\n",
    "    mask_com_risco = gdf_setores['pct_area_impactada_1m'] > 0\n",
    "    gdf_risco_teste = gdf_setores[mask_com_risco].copy()\n",
    "    \n",
    "    if gdf_risco_teste.empty:\n",
    "        print(\"   - Nenhum setor com risco encontrado. Nada a filtrar.\")\n",
    "        ids_validos = []\n",
    "    else:\n",
    "        # Carrega e prepara o oceano\n",
    "        gdf_oceano = gpd.read_file(input_oceano)\n",
    "        if gdf_oceano.crs != gdf_setores.crs:\n",
    "            gdf_oceano = gdf_oceano.to_crs(gdf_setores.crs)\n",
    "\n",
    "        # Define tamanho do buffer\n",
    "        if gdf_setores.crs.is_geographic:\n",
    "            buffer_size = 0.005 # ~500m\n",
    "        else:\n",
    "            buffer_size = 500 # 500m\n",
    "\n",
    "        # Cria buffer único do oceano\n",
    "        geom_oceano = gdf_oceano.dissolve().geometry.buffer(buffer_size).iloc[0]\n",
    "        gdf_oceano_buffer = gpd.GeoDataFrame({'geometry': [geom_oceano]}, crs=gdf_setores.crs)\n",
    "\n",
    "        # Verifica intersecção\n",
    "        gdf_conectados = gpd.sjoin(gdf_risco_teste, gdf_oceano_buffer, how='inner', predicate='intersects')\n",
    "        ids_validos = gdf_conectados['CD_SETOR'].unique()\n",
    "\n",
    "    print(f\"   - Setores validados como costeiros: {len(ids_validos)}\")\n",
    "\n",
    "    # --- ETAPA DE CRIAÇÃO DAS COLUNAS ---\n",
    "    print(\"3. Criando novas colunas...\")\n",
    "\n",
    "    # A. Cria a coluna LIMPA\n",
    "    # Primeiro copiamos tudo da original\n",
    "    gdf_setores['pct_area_impactada_1m_limpo'] = gdf_setores['pct_area_impactada_1m']\n",
    "    \n",
    "    # Identificamos quem deve ser zerado (Tem risco > 0 MAS NÃO está na lista de válidos)\n",
    "    mask_zerar = (gdf_setores['pct_area_impactada_1m'] > 0) & (~gdf_setores['CD_SETOR'].isin(ids_validos))\n",
    "    \n",
    "    # Aplica o zero\n",
    "    gdf_setores.loc[mask_zerar, 'pct_area_impactada_1m_limpo'] = 0.0\n",
    "    print(f\"   - Valores zerados por estarem no interior: {mask_zerar.sum()} setores.\")\n",
    "\n",
    "    # B. Cria a coluna NORMALIZADA (0 a 1)\n",
    "    # Usa a coluna JÁ LIMPA como base\n",
    "    gdf_setores['e3_mar_norm'] = gdf_setores['pct_area_impactada_1m_limpo'] / 100.0\n",
    "    \n",
    "    # Garante que não passe de 1.0 (caso haja erro de arredondamento)\n",
    "    gdf_setores.loc[gdf_setores['e3_mar_norm'] > 1.0, 'e3_mar_norm'] = 1.0\n",
    "\n",
    "    # --- SALVAMENTO ---\n",
    "    print(f\"4. Sobrescrevendo arquivo: {arquivo_alvo} ...\")\n",
    "    # O GeoPandas carrega em memória, então é seguro salvar sobre o mesmo arquivo\n",
    "    gdf_setores.to_file(arquivo_alvo, driver='GPKG')\n",
    "    \n",
    "    print(\"--- Sucesso! ---\")\n",
    "    print(\"Novas colunas adicionadas:\")\n",
    "    print(\"1. 'pct_area_impactada_1m_limpo' (Percentual filtrado)\")\n",
    "    print(\"2. 'e3_mar_norm' (Normalizado 0-1)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    atualizar_arquivo_existente()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef86de7",
   "metadata": {},
   "source": [
    "### 6. Setores: Salva novo arquivo BR_setores_CD2022_e1235_vul_int.gpkg com a coluna 'e3_mar_norm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3f137e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Carregando arquivo BASE (e1, e2, e5, vul_int)...\n",
      "   - Total de setores na base: 468099\n",
      "2. Carregando dados do INDICADOR E3 (Nível do Mar)...\n",
      "   - Dados de E3 carregados para 472780 setores.\n",
      "3. Realizando o cruzamento (Merge)...\n",
      "   - Merge concluído. 0 setores do interior receberam valor 0.0.\n",
      "4. Salvando novo arquivo consolidado...\n",
      "--- SUCESSO! ---\n",
      "Arquivo salvo em: ..\\data\\clean\\sc\\BR_setores_CD2022_e1235_vul_int.gpkg\n",
      "Colunas presentes: ['CD_SETOR', 'id', 'SITUACAO', 'CD_SIT', 'CD_TIPO', 'AREA_KM2', 'CD_REGIAO', 'NM_REGIAO', 'CD_UF', 'NM_UF', 'CD_MUN', 'NM_MUN', 'CD_DIST', 'NM_DIST', 'CD_SUBDIST', 'NM_SUBDIST', 'CD_BAIRRO', 'NM_BAIRRO', 'CD_NU', 'NM_NU', 'CD_FCU', 'NM_FCU', 'CD_AGLOM', 'NM_AGLOM', 'CD_RGINT', 'NM_RGINT', 'CD_RGI', 'NM_RGI', 'CD_CONCURB', 'NM_CONCURB', 'total_enderecos', 'total_enderecos_favelas', 'v2_p_enderecos_fcu', 'e5_media_historica_secas', 'e5_sec_norm', 'v2_mor_norm', 'V00853', 'V00855', 'V00857', 'V00001', 'V00201', 'V00238', 'V01318', 'V01319', 'V01320', 'V01321', 'V01006', 'V01031', 'V01041', 'V01500', 'V03000', 'V01042', 'V01063', 'V06004', 'v1_ren', 'v3_inf', 'v4_edu', 'i1_gen', 'i2_fax', 'i3_cor', 'i4_com', 'v1_ren_norm', 'v3_inf_norm', 'v4_edu_norm', 'i1_gen_norm', 'i2_fax_norm', 'i3_cor_norm', 'i4_com_norm', 'e1_des_pct', 'e1_des_norm', 'e2_inu_pct', 'e2_inu_abs', 'e2_inu_norm', 'ind_vul', 'ind_int', 'geometry', 'e3_mar_norm']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# --- CONFIGURAÇÕES DOS ARQUIVOS ---\n",
    "# Arquivo BASE (que receberá a coluna)\n",
    "input_base_path = os.path.join(\"..\", \"data\", \"clean\", \"sc\", \"BR_setores_CD2022_e125_vul_int.gpkg\")\n",
    "\n",
    "# Arquivo ORIGEM (de onde vem a coluna e3_mar_norm)\n",
    "input_source_path = os.path.join(\"..\", \"data\", \"clean\", \"sc\", \"BR_setores_CD2022_e3.gpkg\")\n",
    "\n",
    "# Arquivo FINAL (Output)\n",
    "output_path = os.path.join(\"..\", \"data\", \"clean\", \"sc\", \"BR_setores_CD2022_e1235_vul_int.gpkg\")\n",
    "\n",
    "def consolidar_indicador_e3():\n",
    "    print(\"1. Carregando arquivo BASE (e1, e2, e5, vul_int)...\")\n",
    "    gdf_base = gpd.read_file(input_base_path)\n",
    "    \n",
    "    # Garante tipagem da chave\n",
    "    if 'CD_SETOR' in gdf_base.columns:\n",
    "        gdf_base['CD_SETOR'] = gdf_base['CD_SETOR'].astype(str)\n",
    "    \n",
    "    print(f\"   - Total de setores na base: {len(gdf_base)}\")\n",
    "\n",
    "    print(\"2. Carregando dados do INDICADOR E3 (Nível do Mar)...\")\n",
    "    # Truque de performance: ignore_geometry=True faz carregar como DataFrame normal (muito mais rápido)\n",
    "    # Se sua versão do geopandas for antiga e der erro, remova o ignore_geometry=True\n",
    "    try:\n",
    "        df_source = gpd.read_file(input_source_path, ignore_geometry=True)\n",
    "    except TypeError:\n",
    "        # Fallback para versões antigas\n",
    "        df_source = gpd.read_file(input_source_path)\n",
    "        df_source = pd.DataFrame(df_source) # Remove parte geo\n",
    "\n",
    "    # Seleciona apenas o que importa para o merge\n",
    "    colunas_interesse = ['CD_SETOR', 'e3_mar_norm']\n",
    "    \n",
    "    # Verifica se a coluna existe mesmo\n",
    "    if 'e3_mar_norm' not in df_source.columns:\n",
    "        raise ValueError(\"A coluna 'e3_mar_norm' não foi encontrada no arquivo de origem!\")\n",
    "        \n",
    "    df_source = df_source[colunas_interesse].copy()\n",
    "    df_source['CD_SETOR'] = df_source['CD_SETOR'].astype(str)\n",
    "    \n",
    "    print(f\"   - Dados de E3 carregados para {len(df_source)} setores.\")\n",
    "\n",
    "    print(\"3. Realizando o cruzamento (Merge)...\")\n",
    "    # Left Join: Mantém todos da base. Quem não tiver correspondência no E3 vira NaN.\n",
    "    gdf_merged = gdf_base.merge(df_source, on='CD_SETOR', how='left')\n",
    "    \n",
    "    # Preenchimento de Nulos:\n",
    "    # Se o setor não estava no arquivo E3 (interior do país), o risco marinho é 0.\n",
    "    nulos_antes = gdf_merged['e3_mar_norm'].isna().sum()\n",
    "    gdf_merged['e3_mar_norm'] = gdf_merged['e3_mar_norm'].fillna(0.0)\n",
    "    \n",
    "    print(f\"   - Merge concluído. {nulos_antes} setores do interior receberam valor 0.0.\")\n",
    "\n",
    "    print(\"4. Salvando novo arquivo consolidado...\")\n",
    "    gdf_merged.to_file(output_path, driver='GPKG')\n",
    "    \n",
    "    print(f\"--- SUCESSO! ---\")\n",
    "    print(f\"Arquivo salvo em: {output_path}\")\n",
    "    print(\"Colunas presentes:\", list(gdf_merged.columns))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    consolidar_indicador_e3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2220340",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bb2656",
   "metadata": {},
   "source": [
    "### 7. Malha H3: Inputar dados do ANADEM na malha H3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72cc9294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando TIFs em: c:\\Users\\CarolinaFaccin\\OneDrive - World Resources Institute\\Carolina WRI\\Repositorios\\climate-justice-index\\data\\raw\\ana\\anadem\\original\\*.tif\n",
      "Arquivos encontrados: 15\n",
      "Criando VRT com caminhos absolutos em: c:\\Users\\CarolinaFaccin\\OneDrive - World Resources Institute\\Carolina WRI\\Repositorios\\climate-justice-index\\data\\raw\\ana\\anadem\\virtual\\anadem_litoral_absoluto.vrt\n",
      "VRT Criado com sucesso.\n",
      "\n",
      "--- TESTE DE LEITURA (PROBE) ---\n",
      "CRS do VRT: EPSG:4326\n",
      "Limites do Raster: BoundingBox(left=-66.00137988094313, bottom=-35.22428976325263, right=-33.90322730642645, top=14.079475111062084)\n",
      "Leitura no Rio de Janeiro (-43.16, -22.91): 0.0 metros\n",
      "SUCESSO: O Raster está lendo dados reais!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from osgeo import gdal\n",
    "import rasterio\n",
    "\n",
    "# --- CONFIGURAÇÕES ---\n",
    "# Caminho da pasta onde estão os TIFs baixados (Raw)\n",
    "pasta_tifs = os.path.join(\"..\", \"data\", \"raw\", \"ana\", \"anadem\", \"original\")\n",
    "# Caminho onde salvaremos o VRT robusto\n",
    "output_vrt = os.path.join(\"..\", \"data\", \"raw\", \"ana\", \"anadem\", \"virtual\", \"anadem_litoral_absoluto.vrt\")\n",
    "\n",
    "def reparar_vrt():\n",
    "    # 1. Obter caminhos ABSOLUTOS dos TIFs\n",
    "    # O abspath resolve o problema de \"onde estou rodando o script\"\n",
    "    search_path = os.path.join(os.path.abspath(pasta_tifs), \"*.tif\")\n",
    "    tif_files = glob.glob(search_path)\n",
    "    \n",
    "    print(f\"Buscando TIFs em: {search_path}\")\n",
    "    print(f\"Arquivos encontrados: {len(tif_files)}\")\n",
    "    \n",
    "    if len(tif_files) == 0:\n",
    "        print(\"ERRO CRÍTICO: Nenhum arquivo .tif encontrado. Verifique o caminho da pasta!\")\n",
    "        return\n",
    "\n",
    "    # 2. Criar diretório do VRT se não existir\n",
    "    pasta_vrt = os.path.dirname(output_vrt)\n",
    "    if not os.path.exists(pasta_vrt):\n",
    "        os.makedirs(pasta_vrt)\n",
    "\n",
    "    # 3. Construir o VRT\n",
    "    print(f\"Criando VRT com caminhos absolutos em: {os.path.abspath(output_vrt)}\")\n",
    "    \n",
    "    # gdal.BuildVRT aceita lista de strings\n",
    "    options = gdal.BuildVRTOptions(resampleAlg='nearest', addAlpha=False)\n",
    "    vrt_ds = gdal.BuildVRT(output_vrt, tif_files, options=options)\n",
    "    vrt_ds = None # Salva e fecha o arquivo\n",
    "    \n",
    "    print(\"VRT Criado com sucesso.\")\n",
    "\n",
    "    # 4. TESTE DE AGULHA (Probe)\n",
    "    # Vamos testar um ponto no Rio de Janeiro (Aeroporto Santos Dumont)\n",
    "    # Lat: -22.91, Lon: -43.16. Altitude deve ser baixa (< 5m).\n",
    "    rio_lon, rio_lat = -43.16, -22.91\n",
    "    \n",
    "    print(\"\\n--- TESTE DE LEITURA (PROBE) ---\")\n",
    "    try:\n",
    "        with rasterio.open(output_vrt) as src:\n",
    "            print(f\"CRS do VRT: {src.crs}\")\n",
    "            print(f\"Limites do Raster: {src.bounds}\")\n",
    "            \n",
    "            # Tenta ler o ponto. O Rasterio espera (Lon, Lat) se o CRS for Geo\n",
    "            # Se for UTM, esse sample vai dar erro ou nodata, mas vamos testar.\n",
    "            try:\n",
    "                val_gen = src.sample([(rio_lon, rio_lat)])\n",
    "                valor = list(val_gen)[0][0]\n",
    "                print(f\"Leitura no Rio de Janeiro ({rio_lon}, {rio_lat}): {valor} metros\")\n",
    "                \n",
    "                if valor == src.nodata or valor < -100:\n",
    "                    print(\"ALERTA: O teste retornou NoData. Verifique se o tile do RJ (23K ou 23J) foi baixado.\")\n",
    "                else:\n",
    "                    print(\"SUCESSO: O Raster está lendo dados reais!\")\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao tentar ler amostra: {e}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao abrir o VRT: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    reparar_vrt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddccf396",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc634ac",
   "metadata": {},
   "source": [
    "### 8. Malha H3: Gerar parquet da malha H3 com input dos dados de elevação do nível do mar\n",
    "Este código calcula o indicador de Suscetibilidade à Elevação do Nível do Mar (e3) para a malha hexagonal H3 (Resolução 9) contendo domicílios. O script classifica como Área de Risco (e3_mar = 1) os hexágonos que atendem simultaneamente a dois critérios:\n",
    "\n",
    "1. Altimetria: Possuem elevação do terreno entre 0 e 1 metro (dados extraídos do MDE ANADEM via amostragem pontual dos centroides).\n",
    "\n",
    "2. Conectividade Costeira: Estão situados a uma distância de até 1.000 metros da linha de costa, incluindo Oceano, Lagunas e Canais (validado via intersecção espacial com o arquivo de buffer pré-processado).\n",
    "\n",
    "Hexágonos que não atendem a ambos os critérios (ex: estão baixos mas no interior do continente, ou estão no litoral mas em morros) recebem valor 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10438fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Carregando H3: ..\\data\\clean\\h3\\br\\br_h3_res9_com_enderecos.parquet\n",
      "2. Convertendo IDs H3 para Lat/Lon...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraindo Geo: 100%|██████████| 4896048/4896048 [00:03<00:00, 1437507.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Amostrando Altitude (ANADEM)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lendo Pixels: 100%|██████████| 10/10 [03:55<00:00, 23.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Filtrando Candidatos (0 <= Alt <= 1)...\n",
      "   - Hexágonos candidatos: 21860\n",
      "5. Validando Conectividade (Arquivo Buffer)...\n",
      "   - Ajustando projeção dos pontos para EPSG:5880...\n",
      "   - Cruzando (Sjoin)...\n",
      "   - Hexágonos CONFIRMADOS: 4442\n",
      "6. Salvando Parquet Final...\n",
      "Concluído.\n",
      "e3_mar\n",
      "0    4891606\n",
      "1       4442\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.warp import transform\n",
    "import h3\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# --- CONFIGURAÇÕES ---\n",
    "vrt_path = os.path.join(\"..\", \"data\", \"raw\", \"ana\", \"anadem\", \"virtual\", \"anadem_litoral_absoluto.vrt\")\n",
    "input_h3_parquet = os.path.join(\"..\", \"data\", \"clean\", \"h3\", \"br\", \"br_h3_res9_com_enderecos.parquet\")\n",
    "# Arquivo do oceano já com buffer feito no QGIS\n",
    "input_oceano_buffer = os.path.join(\"..\", \"data\", \"raw\", \"ibge\", \"hidrografia_2017\", \"br_oceano_lagunas_canais_buffer_1000m.gpkg\")\n",
    "\n",
    "output_h3_parquet = os.path.join(\"..\", \"data\", \"clean\", \"h3\", \"br\", \"br_h3_res9_e3_mar.parquet\")\n",
    "\n",
    "def processar_h3_final_limpo():\n",
    "    print(f\"1. Carregando H3: {input_h3_parquet}\")\n",
    "    df_h3 = pd.read_parquet(input_h3_parquet)\n",
    "    df_h3['e3_mar'] = 0 \n",
    "    \n",
    "    # Identificação robusta da Coluna ID\n",
    "    col_h3 = 'h3_id'\n",
    "    if col_h3 not in df_h3.columns:\n",
    "        if 'h3_address' in df_h3.columns: col_h3 = 'h3_address'\n",
    "        elif df_h3.index.name == 'h3_id': df_h3 = df_h3.reset_index()\n",
    "        else: return print(\"Erro: Coluna ID H3 não encontrada.\")\n",
    "\n",
    "    # --- 2. EXTRAÇÃO DE COORDENADAS ---\n",
    "    print(\"2. Convertendo IDs H3 para Lat/Lon...\")\n",
    "    \n",
    "    def resolver_h3_geo(valor):\n",
    "        s = str(valor).strip()\n",
    "        # Se for numérico, converte para Hex String antes de pedir a geo\n",
    "        if s.isdigit():\n",
    "            try: return h3.cell_to_latlng(h3.int_to_str(int(s)))\n",
    "            except: pass\n",
    "        try: return h3.cell_to_latlng(s)\n",
    "        except: return None, None\n",
    "\n",
    "    lats, lons, indices_validos = [], [], []\n",
    "    vals = df_h3[col_h3].values\n",
    "    \n",
    "    for idx, val in tqdm(enumerate(vals), total=len(vals), desc=\"Extraindo Geo\"):\n",
    "        lat, lon = resolver_h3_geo(val)\n",
    "        if lat is not None:\n",
    "            lats.append(lat)\n",
    "            lons.append(lon)\n",
    "            indices_validos.append(idx)\n",
    "    \n",
    "    if not indices_validos: return print(\"Erro: Nenhuma coordenada extraída.\")\n",
    "\n",
    "    # --- 3. AMOSTRAGEM ANADEM ---\n",
    "    print(\"3. Amostrando Altitude (ANADEM)...\")\n",
    "    altitudes = np.full(len(df_h3), -9999.0)\n",
    "\n",
    "    if not os.path.exists(vrt_path): return print(f\"Erro: VRT não encontrado em {vrt_path}\")\n",
    "\n",
    "    with rasterio.open(vrt_path) as src:\n",
    "        xs, ys = lons, lats\n",
    "        # Reprojeção automática para o sistema do raster se necessário\n",
    "        if src.crs != 'EPSG:4326' and src.crs != 'EPSG:4674':\n",
    "            xs, ys = transform('EPSG:4326', src.crs, lons, lats)\n",
    "        \n",
    "        coords = list(zip(xs, ys))\n",
    "        batch_size = 500000\n",
    "        resultados = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(coords), batch_size), desc=\"Lendo Pixels\"):\n",
    "            gen = src.sample(coords[i : i+batch_size], indexes=1)\n",
    "            resultados.extend([x[0] for x in gen])\n",
    "            \n",
    "        for i, val in enumerate(resultados):\n",
    "            altitudes[indices_validos[i]] = val\n",
    "\n",
    "    df_h3['elevacao_m'] = altitudes\n",
    "\n",
    "    # --- 4. FILTRAGEM ---\n",
    "    print(\"4. Filtrando Candidatos (0 <= Alt <= 1)...\")\n",
    "    mask_cand = (df_h3['elevacao_m'] >= 0.0) & (df_h3['elevacao_m'] <= 1.0)\n",
    "    num_cand = mask_cand.sum()\n",
    "    print(f\"   - Hexágonos candidatos: {num_cand}\")\n",
    "\n",
    "    if num_cand > 0:\n",
    "        print(f\"5. Validando Conectividade (Arquivo Buffer)...\")\n",
    "        gdf_buffer = gpd.read_file(input_oceano_buffer)\n",
    "        \n",
    "        # Prepara geometria apenas dos candidatos para otimizar\n",
    "        df_c = df_h3[mask_cand].copy()\n",
    "        \n",
    "        geoms_c = []\n",
    "        for idx in df_c.index:\n",
    "            val = df_c.loc[idx, col_h3]\n",
    "            lat, lon = resolver_h3_geo(val)\n",
    "            geoms_c.append(Point(lon, lat)) \n",
    "            \n",
    "        # Cria GDF em WGS84\n",
    "        gdf_c = gpd.GeoDataFrame(df_c, geometry=geoms_c, crs=\"EPSG:4326\")\n",
    "        \n",
    "        # Alinha projeção com o arquivo de buffer (seja ele qual for)\n",
    "        if gdf_c.crs != gdf_buffer.crs:\n",
    "            print(f\"   - Ajustando projeção dos pontos para {gdf_buffer.crs}...\")\n",
    "            gdf_c = gdf_c.to_crs(gdf_buffer.crs)\n",
    "\n",
    "        # Cruzamento Espacial\n",
    "        print(\"   - Cruzando (Sjoin)...\")\n",
    "        gdf_join = gpd.sjoin(gdf_c, gdf_buffer, predicate='intersects')\n",
    "        \n",
    "        ids_confirmados = gdf_join[col_h3].unique()\n",
    "        print(f\"   - Hexágonos CONFIRMADOS: {len(ids_confirmados)}\")\n",
    "        \n",
    "        # Atualiza coluna final\n",
    "        df_h3.loc[df_h3[col_h3].isin(ids_confirmados), 'e3_mar'] = 1\n",
    "\n",
    "    # --- 5. SALVAMENTO ---\n",
    "    print(\"6. Salvando Parquet Final...\")\n",
    "    # Formatação de ID para String Hex (Padronização)\n",
    "    df_h3[col_h3] = df_h3[col_h3].apply(lambda x: h3.int_to_str(int(x)) if str(x).isdigit() else str(x))\n",
    "    \n",
    "    df_h3.to_parquet(output_h3_parquet)\n",
    "    print(\"Concluído.\")\n",
    "    print(df_h3['e3_mar'].value_counts())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    processar_h3_final_limpo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed756b4",
   "metadata": {},
   "source": [
    "### 9. Malha H3: Ler parquet para testar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dd96fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo arquivo: ..\\data\\clean\\h3\\br\\br_h3_res9_e3_mar.parquet\n",
      "\n",
      "--- CONTAGEM DE VALORES NA COLUNA 'e3_mar' ---\n",
      "e3_mar\n",
      "0    4891606\n",
      "1       4442\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- ESTATÍSTICAS DA COLUNA 'elevacao_m' PARA OS CASOS DE RISCO ---\n",
      "count    4442.000000\n",
      "mean        0.221344\n",
      "std         0.325272\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.500000\n",
      "max         1.000000\n",
      "Name: elevacao_m, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Caminho do arquivo final que você gerou\n",
    "parquet_path = os.path.join(\"..\", \"data\", \"clean\", \"h3\", \"br\", \"br_h3_res9_e3_mar.parquet\")\n",
    "\n",
    "def verificar_resultado():\n",
    "    print(f\"Lendo arquivo: {parquet_path}\")\n",
    "    \n",
    "    if not os.path.exists(parquet_path):\n",
    "        print(\"Erro: Arquivo não encontrado.\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    \n",
    "    print(\"\\n--- CONTAGEM DE VALORES NA COLUNA 'e3_mar' ---\")\n",
    "    contagem = df['e3_mar'].value_counts()\n",
    "    print(contagem)\n",
    "    \n",
    "    print(\"\\n--- ESTATÍSTICAS DA COLUNA 'elevacao_m' PARA OS CASOS DE RISCO ---\")\n",
    "    # Filtra só quem é risco 1 para ver se a altitude faz sentido\n",
    "    risco = df[df['e3_mar'] == 1]\n",
    "    print(risco['elevacao_m'].describe())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    verificar_resultado()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebba88b3",
   "metadata": {},
   "source": [
    "### 10. Malha H3: Merge das camadas '_e3_mar.parquet' com '_e125_vul_int.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9daacdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Lendo base (e1, e2, e5, vul_int): ..\\data\\clean\\h3\\br\\br_h3_res9_e125_vul_int.parquet\n",
      "   - Total de hexágonos na base: 4893154\n",
      "2. Lendo indicador E3 (Nível do Mar): ..\\data\\clean\\h3\\br\\br_h3_res9_e3_mar.parquet\n",
      "3. Realizando Merge (Left Join)...\n",
      "   - Hexágonos do interior preenchidos com 0: 0\n",
      "   - Hexágonos com risco confirmado (1): 4250\n",
      "4. Salvando arquivo final: ..\\data\\clean\\h3\\br\\br_h3_res9_e1235_vul_int.parquet\n",
      "Concluído! Colunas no arquivo final:\n",
      "['h3_id', 'qtd_enderecos', 'CD_MUN', 'NM_MUN', 'CD_SETOR', '__index_level_0__', 'e5_sec_norm', 'ind_vul', 'ind_int', 'e1_des_norm', 'e2_inu_norm', 'e3_mar']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# --- CONFIGURAÇÕES ---\n",
    "input_base = os.path.join(\"..\", \"data\", \"clean\", \"h3\", \"br\", \"br_h3_res9_e125_vul_int.parquet\")\n",
    "input_e3 = os.path.join(\"..\", \"data\", \"clean\", \"h3\", \"br\", \"br_h3_res9_e3_mar.parquet\")\n",
    "output_final = os.path.join(\"..\", \"data\", \"clean\", \"h3\", \"br\", \"br_h3_res9_e1235_vul_int.parquet\")\n",
    "\n",
    "def consolidar_h3_final():\n",
    "    print(f\"1. Lendo base (e1, e2, e5, vul_int): {input_base}\")\n",
    "    df_base = pd.read_parquet(input_base)\n",
    "    \n",
    "    # Normalização de ID (Base)\n",
    "    col_id_base = 'h3_id'\n",
    "    if col_id_base not in df_base.columns:\n",
    "        # Tenta achar nomes comuns\n",
    "        found = False\n",
    "        for c in ['h3_address', 'hex_id', 'h3_index']:\n",
    "            if c in df_base.columns:\n",
    "                df_base.rename(columns={c: 'h3_id'}, inplace=True)\n",
    "                found = True\n",
    "                break\n",
    "        if not found and df_base.index.name in ['h3_id', 'h3_address', 'h3_index']:\n",
    "            df_base = df_base.reset_index()\n",
    "            # Se o index não tinha nome, renomeia a coluna criada 'index'\n",
    "            if 'h3_id' not in df_base.columns: df_base.rename(columns={'index': 'h3_id'}, inplace=True)\n",
    "\n",
    "    print(f\"   - Total de hexágonos na base: {len(df_base)}\")\n",
    "\n",
    "    print(f\"2. Lendo indicador E3 (Nível do Mar): {input_e3}\")\n",
    "    df_e3 = pd.read_parquet(input_e3)\n",
    "    \n",
    "    # Normalização de ID (E3)\n",
    "    col_id_e3 = 'h3_id'\n",
    "    if col_id_e3 not in df_e3.columns:\n",
    "        for c in ['h3_address', 'hex_id', 'h3_index']:\n",
    "            if c in df_e3.columns:\n",
    "                df_e3.rename(columns={c: 'h3_id'}, inplace=True)\n",
    "                break\n",
    "        if 'h3_id' not in df_e3.columns and df_e3.index.name in ['h3_id', 'h3_index']:\n",
    "            df_e3 = df_e3.reset_index()\n",
    "            if 'h3_id' not in df_e3.columns: df_e3.rename(columns={'index': 'h3_id'}, inplace=True)\n",
    "\n",
    "    # Seleciona apenas as colunas necessárias para o merge\n",
    "    # (ID e a coluna de interesse)\n",
    "    if 'e3_mar' not in df_e3.columns:\n",
    "        print(\"ERRO: Coluna 'e3_mar' não encontrada no arquivo de input.\")\n",
    "        return\n",
    "        \n",
    "    df_e3_clean = df_e3[['h3_id', 'e3_mar']].copy()\n",
    "    \n",
    "    # Garante que os IDs sejam string para o merge funcionar\n",
    "    df_base['h3_id'] = df_base['h3_id'].astype(str)\n",
    "    df_e3_clean['h3_id'] = df_e3_clean['h3_id'].astype(str)\n",
    "\n",
    "    print(\"3. Realizando Merge (Left Join)...\")\n",
    "    # Mantemos a base completa (left). Quem não tem dado de mar vira NaN (e depois 0).\n",
    "    df_final = df_base.merge(df_e3_clean, on='h3_id', how='left')\n",
    "    \n",
    "    # Preenche NaNs com 0 (quem não cruzou é área sem risco ou interior)\n",
    "    nulos = df_final['e3_mar'].isna().sum()\n",
    "    df_final['e3_mar'] = df_final['e3_mar'].fillna(0).astype(int)\n",
    "    \n",
    "    print(f\"   - Hexágonos do interior preenchidos com 0: {nulos}\")\n",
    "    print(f\"   - Hexágonos com risco confirmado (1): {df_final['e3_mar'].sum()}\")\n",
    "\n",
    "    print(f\"4. Salvando arquivo final: {output_final}\")\n",
    "    df_final.to_parquet(output_final)\n",
    "    print(\"Concluído! Colunas no arquivo final:\")\n",
    "    print(df_final.columns.tolist())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    consolidar_h3_final()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
