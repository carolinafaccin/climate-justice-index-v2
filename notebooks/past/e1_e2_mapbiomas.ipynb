{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "304a1e17",
   "metadata": {},
   "source": [
    "## 1/2 -- Inputar dados do MapBiomas na malha de setores censitários"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0f4154",
   "metadata": {},
   "source": [
    "### A) Exposição a deslizamentos -- Área afetada -- Converter para EPSG 5880"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43c7917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Carregando malha de setores...\n",
      "   CRS original do vetor: EPSG:4674\n",
      "2. Verificando CRS do Raster e alinhando vetor...\n",
      "   CRS do Raster: EPSG:4326\n",
      "3. Calculando estatísticas zonais (Deslizamentos)...\n",
      "4. Normalizando indicador...\n",
      "5. Convertendo para EPSG:5880 e salvando...\n",
      "Concluído Código 1.\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterstats import zonal_stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# --- CONFIGURAÇÃO ---\n",
    "input_gpkg = '../data/clean/BR_setores_CD2022_e5_v1234_i1234_norm.gpkg' \n",
    "raster_deslizamentos = '../data/raw/mapbiomas/mapbiomas_landslides_2024.tif'\n",
    "output_step1 = '../data/raw/mapbiomas/temp_step1_deslizamentos_5880.gpkg'\n",
    "\n",
    "# EPSG Desejado para o arquivo final (Brazil Polyconic)\n",
    "target_epsg = 5880\n",
    "\n",
    "print(\"1. Carregando malha de setores...\")\n",
    "gdf = gpd.read_file(input_gpkg)\n",
    "print(f\"   CRS original do vetor: {gdf.crs}\")\n",
    "\n",
    "# --- AJUSTE DE CRS PARA CÁLCULO ---\n",
    "print(\"2. Verificando CRS do Raster e alinhando vetor...\")\n",
    "with rasterio.open(raster_deslizamentos) as src:\n",
    "    raster_crs = src.crs\n",
    "    print(f\"   CRS do Raster: {raster_crs}\")\n",
    "    \n",
    "    # Reprojetar o vetor para o MESMO sistema do raster temporariamente\n",
    "    # Isso é obrigatório para o zonal_stats funcionar\n",
    "    gdf_calc = gdf.to_crs(raster_crs)\n",
    "\n",
    "# --- CÁLCULO (ZONAL STATS) ---\n",
    "print(\"3. Calculando estatísticas zonais (Deslizamentos)...\")\n",
    "# stats='mean' em um raster binário (0 e 1) retorna a porcentagem de área coberta (0 a 1)\n",
    "stats = zonal_stats(gdf_calc, raster_deslizamentos, stats=['mean'])\n",
    "df_stats = pd.DataFrame(stats)\n",
    "\n",
    "# Preencher nulos com 0 (caso o setor não toque no raster)\n",
    "df_stats['mean'] = df_stats['mean'].fillna(0)\n",
    "\n",
    "# Passar os dados de volta para o GeoDataFrame original (ou para o reprojetado, tanto faz)\n",
    "# Vamos usar o gdf_calc e depois converter para o final\n",
    "gdf_calc['e_des_pct'] = df_stats['mean'] \n",
    "\n",
    "# --- NORMALIZAÇÃO ---\n",
    "print(\"4. Normalizando indicador...\")\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "gdf_calc['e_des_norm'] = scaler.fit_transform(gdf_calc['e_des_pct'].values.reshape(-1, 1))\n",
    "\n",
    "# --- CONVERSÃO FINAL PARA EPSG 5880 E SALVAMENTO ---\n",
    "print(f\"5. Convertendo para EPSG:{target_epsg} e salvando...\")\n",
    "gdf_final = gdf_calc.to_crs(epsg=target_epsg)\n",
    "\n",
    "gdf_final.to_file(output_step1, driver='GPKG')\n",
    "print(\"Concluído Código 1.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da6a9e2",
   "metadata": {},
   "source": [
    "### B) Exposição a Inundações - Ponderado pelo número de domicílios -- Converter para EPSG 5880"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6f6db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Carregando malha do passo anterior...\n",
      "2. Verificando CRS do Raster de Inundações...\n",
      "   CRS do Raster: EPSG:4326\n",
      "3. Calculando área inundada...\n",
      "4. Ponderando pela densidade de domicílios...\n",
      "5. Normalizando indicador de inundações...\n",
      "6. Revertendo para EPSG:5880 (Brazil Polyconic) e salvando...\n",
      "Processo finalizado! Arquivo salvo em: ../data/clean/BR_setores_CD2022_e125_v1234_i1234_norm.gpkg\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterstats import zonal_stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# --- CONFIGURAÇÃO ---\n",
    "input_gpkg = '../data/raw/mapbiomas/temp_step1_deslizamentos_5880.gpkg' \n",
    "raster_inundacoes = '../data/raw/mapbiomas/mapbiomas_floods_2024.tif'\n",
    "output_gpkg_final = '../data/clean/BR_setores_CD2022_e125_v1234_i1234_norm.gpkg'\n",
    "target_epsg = 5880 # Brazil Polyconic\n",
    "\n",
    "print(\"1. Carregando malha do passo anterior...\")\n",
    "gdf = gpd.read_file(input_gpkg)\n",
    "\n",
    "# --- AJUSTE DE CRS PARA CÁLCULO ---\n",
    "print(\"2. Verificando CRS do Raster de Inundações...\")\n",
    "with rasterio.open(raster_inundacoes) as src:\n",
    "    raster_crs = src.crs\n",
    "    print(f\"   CRS do Raster: {raster_crs}\")\n",
    "    \n",
    "    # O arquivo de entrada está em 5880, precisamos converter para o do raster (ex: 4326)\n",
    "    # para o cálculo geométrico bater\n",
    "    gdf_calc = gdf.to_crs(raster_crs)\n",
    "\n",
    "# --- CÁLCULO (ZONAL STATS) ---\n",
    "print(\"3. Calculando área inundada...\")\n",
    "stats_floods = zonal_stats(gdf_calc, raster_inundacoes, stats=['mean'])\n",
    "df_floods = pd.DataFrame(stats_floods)\n",
    "df_floods['mean'] = df_floods['mean'].fillna(0)\n",
    "\n",
    "gdf_calc['e_inu_pct'] = df_floods['mean']\n",
    "\n",
    "# --- PONDERAÇÃO POR DOMICÍLIOS ---\n",
    "# Importante: Como 'V00001' é um número absoluto (contagem), ele independe do CRS.\n",
    "coluna_total_domicilios = 'V00001' \n",
    "\n",
    "if coluna_total_domicilios in gdf_calc.columns:\n",
    "    print(\"4. Ponderando pela densidade de domicílios...\")\n",
    "    # % da área alagada * Quantidade de casas no setor = Estimativa de casas atingidas\n",
    "    gdf_calc['e_inu_abs'] = gdf_calc['e_inu_pct'] * gdf_calc[coluna_total_domicilios]\n",
    "else:\n",
    "    print(f\"ATENÇÃO: Coluna {coluna_total_domicilios} não encontrada. Usando valor bruto.\")\n",
    "    gdf_calc['e_inu_abs'] = gdf_calc['e_inu_pct']\n",
    "\n",
    "# --- NORMALIZAÇÃO ---\n",
    "print(\"5. Normalizando indicador de inundações...\")\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "gdf_calc['e_inu_norm'] = scaler.fit_transform(gdf_calc['e_inu_abs'].values.reshape(-1, 1))\n",
    "\n",
    "# --- FINALIZAÇÃO ---\n",
    "print(f\"6. Revertendo para EPSG:{target_epsg} (Brazil Polyconic) e salvando...\")\n",
    "gdf_final = gdf_calc.to_crs(epsg=target_epsg)\n",
    "\n",
    "# Limpeza opcional de colunas intermediárias para o arquivo não ficar gigante\n",
    "# cols_drop = ['e_des_pct', 'e_inu_pct', 'e_inu_abs', 'mean']\n",
    "# gdf_final = gdf_final.drop(columns=cols_drop, errors='ignore')\n",
    "\n",
    "gdf_final.to_file(output_gpkg_final, driver='GPKG')\n",
    "print(f\"Processo finalizado! Arquivo salvo em: {output_gpkg_final}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ba8c56",
   "metadata": {},
   "source": [
    "## 2/2 -- Inputar dados do MapBiomas no H3 usando DuckDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7bbcd5",
   "metadata": {},
   "source": [
    "### A) Inputar dados de deslizamentos do Mapbiomas no .parquet do H3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9522df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PROCESSANDO COORDENADAS COM DUCKDB ---\n",
      "Amostrando raster...\n",
      "Finalizando cálculos e salvando...\n",
      "Concluído!\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "\n",
    "# --- CONFIGURAÇÕES ---\n",
    "CAMINHO_H3_INPUT = '../data/clean/h3/br/br_h3_res9_com_setores.parquet'\n",
    "RASTER_DESLIZAMENTOS = '../data/raw/mapbiomas/mapbiomas_landslides_2024.tif'\n",
    "ARQUIVO_SAIDA = '../data/clean/h3/br/br_h3_res9_e1_deslizamento.parquet'\n",
    "\n",
    "# 1. Conectar ao DuckDB\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Tenta instalar do repositório da comunidade (onde o H3 vive hoje)\n",
    "try:\n",
    "    con.install_extension(\"h3\", repository=\"community\")\n",
    "    con.load_extension(\"h3\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar H3: {e}\")\n",
    "    # Fallback caso a versão comunidade falhe:\n",
    "    con.sql(\"SET custom_extension_repository='http://extensions.duckdb.org/community';\")\n",
    "    con.sql(\"INSTALL h3;\")\n",
    "    con.load_extension(\"h3\")\n",
    "\n",
    "print(\"--- PROCESSANDO COORDENADAS COM DUCKDB ---\")\n",
    "\n",
    "# 2. Extrair Lat/Lon usando SQL (Muito mais rápido que loop h3.cell_to_latlng)\n",
    "# h3_cell_to_lat retorna um struct {lat, lng}\n",
    "df_coords = con.sql(f\"\"\"\n",
    "    SELECT \n",
    "        *,\n",
    "        h3_cell_to_lat(h3_id) as lat,\n",
    "        h3_cell_to_lng(h3_id) as lon\n",
    "    FROM read_parquet('{CAMINHO_H3_INPUT}')\n",
    "\"\"\").df()\n",
    "\n",
    "# 3. Amostragem do Raster (Rasterio continua sendo a melhor ferramenta aqui)\n",
    "print(\"Amostrando raster...\")\n",
    "with rasterio.open(RASTER_DESLIZAMENTOS) as src:\n",
    "    # Passamos as colunas do dataframe diretamente para o gerador de amostras\n",
    "    # Invertemos para (lon, lat) conforme exigido pelo rasterio para EPSG:4326\n",
    "    coord_samples = zip(df_coords['lon'], df_coords['lat'])\n",
    "    \n",
    "    # Atribuição direta (o list comprehension aqui é rápido)\n",
    "    df_coords['e_des_val'] = [val[0] for val in src.sample(coord_samples)]\n",
    "\n",
    "# 4. Normalização com SQL (DuckDB é excelente para cálculos matemáticos)\n",
    "# Registramos o dataframe como uma tabela temporária para usar SQL de novo\n",
    "con.register('df_final', df_coords)\n",
    "\n",
    "print(\"Finalizando cálculos e salvando...\")\n",
    "con.sql(\"\"\"\n",
    "    COPY (\n",
    "        SELECT \n",
    "            *,\n",
    "            CASE \n",
    "                WHEN max(e_des_val) OVER () > 1 \n",
    "                THEN (e_des_val - min(e_des_val) OVER ()) / (NULLIF(max(e_des_val) OVER () - min(e_des_val) OVER (), 0))\n",
    "                ELSE e_des_val \n",
    "            END as e_des_norm\n",
    "        FROM df_final\n",
    "    ) TO '{ARQUIVO_SAIDA}' (FORMAT PARQUET)\n",
    "\"\"\".format(ARQUIVO_SAIDA=ARQUIVO_SAIDA))\n",
    "\n",
    "print(\"Concluído!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e82bb8",
   "metadata": {},
   "source": [
    "### B) Inputar dados de inundações do Mapbiomas no .parquet do H3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8079b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CONFIGURANDO EXTENSÃO H3 ---\n",
      "--- EXTRAINDO COORDENADAS COM DUCKDB ---\n",
      "Amostrando raster de inundações...\n",
      "Calculando ponderação (endereços) e normalização...\n",
      "Concluído! Arquivo salvo com sucesso em: ../data/clean/h3/br_h3_res9_exposicao_inundacao.parquet\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- CONFIGURAÇÕES ---\n",
    "CAMINHO_H3_INPUT = '../data/clean/h3/br_h3_res9_com_setores.parquet'\n",
    "RASTER_INUNDACOES = '../data/raw/mapbiomas/mapbiomas_floods_2024.tif'\n",
    "ARQUIVO_SAIDA = '../data/clean/h3/br_h3_res9_e2_inundacao.parquet'\n",
    "\n",
    "# 1. Conexão e Configuração da Extensão H3\n",
    "con = duckdb.connect()\n",
    "\n",
    "print(\"--- CONFIGURANDO EXTENSÃO H3 ---\")\n",
    "# Ajuste vital para evitar o erro 404 no Windows\n",
    "con.sql(\"SET custom_extension_repository='http://extensions.duckdb.org/community';\")\n",
    "con.sql(\"INSTALL h3; LOAD h3;\")\n",
    "\n",
    "print(\"--- EXTRAINDO COORDENADAS COM DUCKDB ---\")\n",
    "\n",
    "# 2. SQL para Lat/Lon\n",
    "# O DuckDB processa os 5 milhões de registros muito mais rápido que o Python puro\n",
    "df_temp = con.sql(f\"\"\"\n",
    "    SELECT \n",
    "        h3_id, \n",
    "        qtd_enderecos,\n",
    "        h3_cell_to_lat(h3_id) as lat,\n",
    "        h3_cell_to_lng(h3_id) as lon\n",
    "    FROM read_parquet('{CAMINHO_H3_INPUT}')\n",
    "\"\"\").df()\n",
    "\n",
    "# 3. Amostragem do Raster (Rasterio)\n",
    "print(\"Amostrando raster de inundações...\")\n",
    "with rasterio.open(RASTER_INUNDACOES) as src:\n",
    "    # Gerador eficiente para não estourar a RAM\n",
    "    coord_samples = zip(df_temp['lon'], df_temp['lat'])\n",
    "    df_temp['e_inu_val'] = [val[0] for val in src.sample(coord_samples)]\n",
    "\n",
    "# 4. Cálculo do Índice e Normalização no DuckDB\n",
    "con.register('dados_amostrados', df_temp)\n",
    "\n",
    "print(\"Calculando ponderação (endereços) e normalização...\")\n",
    "# Usamos SQL para cálculos de coluna (vetorizados)\n",
    "con.sql(f\"\"\"\n",
    "    COPY (\n",
    "        WITH calculo_base AS (\n",
    "            SELECT \n",
    "                *,\n",
    "                -- Multiplica a presença de inundação pela quantidade de endereços\n",
    "                COALESCE(e_inu_val, 0) * qtd_enderecos AS e_inu_abs\n",
    "            FROM dados_amostrados\n",
    "        )\n",
    "        SELECT \n",
    "            *,\n",
    "            -- Normalização Min-Max (0 a 1) sobre o valor absoluto\n",
    "            (e_inu_abs - min(e_inu_abs) OVER ()) / \n",
    "            NULLIF(max(e_inu_abs) OVER () - min(e_inu_abs) OVER (), 0) AS e_inu_norm\n",
    "        FROM calculo_base\n",
    "    ) TO '{ARQUIVO_SAIDA}' (FORMAT PARQUET)\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Concluído! Arquivo salvo com sucesso em: {ARQUIVO_SAIDA}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
